# Insurance OCR Normalization — Hybrid LLM + Minimal Code Approach
### Context for Cursor IDE Implementation

---

# Overview

Insurance documents vary widely in structure, formatting, quality, and content.  
Traditional rule-based normalization becomes unmaintainable due to:

- Unpredictable OCR artifacts  
- Different insurer templates  
- Broken tables, hyphenation, spacing issues  
- Non-standard formatting  
- Multilingual scans  
- Handwritten fields

**A hybrid approach using an LLM for structural cleanup + minimal Python code for deterministic normalization delivers the best accuracy with the least complexity.**

This document describes:

1. Why LLM-driven normalization is recommended  
2. What fields must be normalized  
3. The normalization pipeline architecture  
4. A ready-to-use LLM normalization prompt  
5. The full implementation plan for your codebase  

It is designed to be used directly inside **Cursor IDE** to build the implementation.

---

# Why NOT Use Rule-Based Normalization?

Traditional rule-based pipelines require thousands of lines of code:

- Regex for hyphenation  
- Word-merge detection  
- Table reconstruction  
- List normalization  
- Broken paragraphs  
- LaTeX symbol cleanup  
- Escape character repair  
- Markdown corrections  

And all of these vary per document type.

**This does not scale** and creates massive maintenance overhead.

---

# Recommended Architecture: Hybrid LLM Normalization Pipeline

Mistral OCR → LLM Text Normalizer → Semantic Normalizer → Final JSON Output

### ✔ LLM performs:
- Text cleanup
- Hyphenation repair
- Markdown cleanup
- Table reconstruction
- Line break + paragraph fixing
- Removing OCR artifacts
- Restoring numbering & structure
- Ensuring readable, consistent text

### ✔ Code performs:
- Date normalization  
- Currency normalization  
- Amount parsing  
- Policy number validation  
- Entity extraction (NER/regex hybrid)  
- Business rule validation  
- Schema mapping  

This minimizes code, maximizes accuracy, and works on ANY insurance document.

---

# Stage 1 — LLM Text Normalization (Critical Step)

Use an LLM to normalize the raw text from OCR.

## LLM Responsibilities

The LLM will:

- Fix merged words  
- Fix word splits & hyphenation  
- Remove LaTeX fragments  
- Remove OCR artifacts like `\\%`, `$75 \\%$`, `) .`, etc.  
- Insert missing spaces  
- Standardize markdown headers  
- Normalize tables  
- Reconstruct bullet/number lists  
- Remove superfluous whitespace  
- Produce clean markdown, preserving exact meaning  

This eliminates 80–90% of code you would otherwise need.

---

# Recommended LLM Prompt (Copy-paste Ready)

You are an expert in insurance document OCR cleanup.

Your task is to transform raw OCR text into clean, normalized markdown
WITHOUT changing the meaning or rewriting the content.

Perform the following normalization steps:
1. Fix broken words, hyphenation, and merged tokens.
2. Remove OCR artifacts such as:
   • backslashes  
   • LaTeX fragments ($…$, \%)  
   • unnecessary escape characters
3. Normalize percentages (e.g., “$75 %$” → “75%”).
4. Normalize tables:
   • Merge broken rows  
   • Clean header formatting  
   • Ensure | col | col | structure
5. Normalize markdown:
   • Ensure proper headers (#, ##)  
   • Insert line breaks after headings  
   • Clean bullet and numbering lists (1., 2., i., ii.)
6. Reconstruct paragraphs with correct spacing.
7. Ensure the output is clean markdown with:
   • No hallucinations  
   • No new text  
   • No semantic alterations

Use this prompt inside your LLM normalizer class.

---

# Stage 2 — Deterministic Normalization (Minimal Code Layer)

Use Python to normalize strictly structured fields after LLM cleanup.

## **Fields that MUST be normalized (Insurance-Specific)**

### **Dates**
Normalize to:
```
YYYY-MM-DD
```

Handles:
- "12th Dec 2023"
- "12/12/23"
- “12.12.2023”
- “12th December, 2023”

### **Monetary Amounts**
Normalize to:

```
float / integer
```

Handles:
- ₹1,20,000  
- Rs. 12,500/-  
- 25,00,000.00  
- USD $500.00  

### **Percentages**
Normalize:
- `75 %`, `75%`, `$75 \\%$`, `75 \%` → `75%`

### **Policy Number**
Normalize:
- Remove spaces
- Uppercase
- Remove trailing special characters
- Validate insurer formats later

### **Claim Number / Certificate Number**
Same as policy number normalization.

### **Names**
- Title case
- Remove double spaces

### **Addresses**
- Flatten multi-line OCR addresses
- Remove broken punctuation
- Normalize spacing

### **Table Values**
Flatten table cells into:

```
{
“age_of_vehicle”: “2 to 3 years”,
“depreciation”: “20%”
}
```

### **Sum Insured / Premium / IDV**
Convert to numeric:

```
“IDV”: 470000
```

### **Phone, Email, PAN, Aadhaar**
Standardize validation & formatting.

---

# Stage 3 — Final Semantic Extraction

After normalization, extract structured insurance fields.

Examples:

{
  "policy_number": "...",
  "insured_name": "...",
  "period_of_insurance": {
    "start": "...",
    "end": "..."
  },
  "premium_total": 12345,
  "sum_insured": 500000,
  "vehicle_details": {
    "make": "...",
    "model": "...",
    "year": "...",
    "engine_number": "...",
    "chassis_number": "..."
  }
}

Use:

- Lightweight regex  
- Named Entity Recognition  
- Small insurance-specific dictionaries  
- Optional LLM field extraction  

Minimal code required.

---

# Implementation Structure for Cursor IDE

Key files:

### ✔ llm_normalizer.py → **Performs LLM-based cleanup**  
### ✔ semantic_normalizer.py → **Deterministic field normalization**  
### ✔ pipeline.py → **Orchestrates normalization steps**

---

# High-Level Pipeline

```python
raw_text = ocr_service.extract_text_from_url(url)

normalized_text = LLMTextNormalizer().normalize(raw_text)

final_structured_json = InsuranceSemanticNormalizer().process(normalized_text)
```

⚡ Benefits of the Hybrid Approach

| Approach                                 | Accuracy      | Code Required | Scalability | Maintenance  |
| ---------------------------------------- | ------------- | ------------- | ----------- | ------------ |
| Hand-coded regex                         | Medium        | Massive       | Low         | High         |
| Rule-based NLP                           | Medium        | High          | Low         | Hard         |
| Layout models                            | High          | Very high     | Medium      | Hard         |
| **LLM text normalization (recommended)** | **Very high** | **Minimal**   | **High**    | **Very low** |
| Hybrid (LLM + code)                      | **Best**      | **Low**       | **Best**    | **Best**     |

---

Conclusion

This hybrid normalization approach:
• Dramatically reduces code complexity  
• Works across all insurers and document types  
• Achieves high-quality, consistent normalization  
• Offloads structure recovery to the LLM  
• Leaves only canonical field normalization to code
