# Document Classification — Normalization + Metadata Extraction Per Chunk → Global Aggregation
**Context:** replaces the earlier zero-shot-only classification plan. This file defines a robust, cost-efficient approach that **extracts classification signals while normalizing each chunk**, then **aggregates** those signals into a final document label. Designed for integration with your existing OCR → chunking → Temporal pipeline and PostgreSQL schema.

---

## Goals
- Reduce redundant LLM calls (no separate full-document LLM classification unless necessary).
- Keep classification accurate by combining many local signals into a global decision.
- Make the pipeline resilient, auditable and cheap (use rules + per-chunk LLM signal extraction + lightweight aggregator).
- Integrate tightly with Temporal and DB for retries, observability and human review.

---

## High-level flow (final)
Insurance Document
↓ OCR (Mistral OCR)
↓ Page & Section Chunking
↓ Per-Chunk: LLM Normalization + Metadata Extraction (signals)
↓ Persist per-chunk normalized text + signals
↓ Aggregation Service (compute global label & confidence)
├─ If high-confidence → accept label
└─ Else → one small LLM classification call on aggregated summary (fallback)
↓ Insert into DB (document_classifications) & update document status
↓ Trigger downstream: vector indexing, KG updates, workflows

---

## Why this approach?
- **Chunk LLM calls** are required (token limits). While normalizing chunks, ask the LLM to also emit *classification signals* for that chunk (probabilities, keywords, entity presence).
- **Aggregate** chunk signals to get a robust document-level decision without reading the entire document again with an expensive LLM call.
- **Fallback**: only when aggregated confidence is low do we make a single small LLM call on a merged summary (cheap).

---

## Data model additions (DB)
Add (or reuse) tables to persist per-chunk outputs and signals.

### `document_chunks`
Store chunk metadata & normalized text (if not already present).
```sql
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY,
  document_id UUID REFERENCES documents(id),
  page_number INT,
  section_name TEXT,
  chunk_index INT,          -- order within page/section
  raw_text TEXT,            -- original chunk text
  normalized_text TEXT,     -- LLM-normalized chunk text
  token_count INT,
  created_at TIMESTAMP DEFAULT NOW()
);

### `chunk_classification_signals`
Store per-chunk signal outputs from the LLM (or rules).
```sql
CREATE TABLE chunk_classification_signals (
  id UUID PRIMARY KEY,
  chunk_id UUID REFERENCES document_chunks(id),
  signals JSONB,            -- e.g. {"policy":0.12,"claim":0.78,...}
  keywords JSONB,           -- extracted keywords / triggers
  entities JSONB,           -- important entities (policy_no, claim_no, dates)
  model_name TEXT,
  model_confidence NUMERIC, -- optional raw confidence summary
  created_at TIMESTAMP DEFAULT NOW()
);
```
document_classifications (update if needed)

Store final aggregated classification:
```
-- existing table, ensure fields:
-- document_id, classified_type, confidence, classifier_model, created_at
```

Per-chunk LLM prompt & expected output

Prompt goals when calling LLM for each chunk:
	1.	Normalize text (fix hyphenation, remove artifacts).
	2.	Extract lightweight classification signals (per-class confidence scores or logits-like numbers).
	3.	Extract keywords and entities helpful for aggregation and downstream extraction.
	4.	Keep response compact and machine-parsable (JSON).

Chunk prompt template (example):

You are an OCR normalizer and metadata extractor specialized in insurance documents.

TASK:
1) Normalize the following OCR chunk (fix broken words, hyphenation, remove obvious artifacts, keep tables readable).
2) Extract classification signals for the following classes:
   [policy, claim, submission, quote, proposal, SOV, financials, loss_run, audit, endorsement, invoice, correspondence]
   For each class, provide a numeric score 0.0-1.0 indicating how strongly this chunk suggests that class.
3) Return up to 8 short keywords or phrases from the chunk that indicate document type (e.g., "Loss Date", "Declarations Page", "Policy Number").
4) Extract key entities if present (policy_number, claim_number, insured_name, dates, amounts) and their normalized forms.

RETURN ONLY JSON with exactly these keys:
{
  "normalized_text": "...",
  "signals": {"policy":0.12, "claim":0.78, ...},
  "keywords": ["loss date", "insured", "claim number"],
  "entities": {"policy_number":"ABC123", "loss_date":"2024-01-12"}
}
CHUNK:
{{chunk_text}}

Note: Use the same system prompt + template for every chunk to keep signals calibrated.

---

Aggregation algorithm (server-side)

After collecting all signals JSON objects for a document:
1. Sum or average class scores across chunks. Example:

doc_scores[class] = SUM(chunk_signals[class] * chunk_weight)
•	chunk_weight can be uniform or based on chunk confidence/length/importance (e.g., Declarations page higher weight).

2. Normalize scores to 0–1:

normalized_score[class] = doc_scores[class] / max_possible_score

3.	Select top class and confidence = normalized_score[top].
4.	Threshold decision:
	•	If confidence >= ACCEPT_THRESHOLD (e.g., 0.70–0.85) → accept.
	•	Else if confidence >= REVIEW_THRESHOLD (e.g., 0.45–0.70) → route for lightweight fallback (single LLM classification on aggregated summary) or human review depending on your SLA.
	•	Else → low confidence → human review.
5.	Fallback (if used): build a short aggregated summary from chunk keywords + short per-chunk summaries or top N normalized_text snippets (limit to ~1500 tokens), then call a single LLM classification prompt on this summary (cheap relative to classifying whole doc).
6.	Final persistence: insert document_classifications with classified_type, confidence, classifier_model = "chunk_aggregator_v1" (or "llm_fallback_v1" if fallback used).

---

## Aggregation pseudocode
```python
def aggregate_signals(chunk_signals_list, weights=None):
    # chunk_signals_list: list of dicts like {"policy":0.1, "claim":0.7, ...}
    classes = all_possible_classes()
    totals = {c: 0.0 for c in classes}
    for i, s in enumerate(chunk_signals_list):
        w = weights[i] if weights else 1.0
        for c in classes:
            totals[c] += s.get(c, 0.0) * w
    # normalize
    max_total = max(totals.values()) or 1.0
    normalized = {c: totals[c] / max_total for c in classes}
    top_class, top_score = max(normalized.items(), key=lambda x: x[1])
    return top_class, top_score, normalized
```

---

Chunk weighting heuristics
	•	Page importance: give higher weight to pages that look like Declarations, first pages, or ones that contain policy numbers.
	•	Chunk confidence: if LLM returns a model_confidence value for normalized_text quality, use it to weight signal.
	•	Keyword triggers: if chunk contains clear trigger keywords (e.g., “Loss Date”, “Claim Number”), apply multiplier.

---

Storage & observability
	•	Persist all per-chunk normalized_text and signals into document_chunks and chunk_classification_signals.
	•	Log aggregation inputs and outputs to workflow_run_events with event_type=classification_aggregation.
	•	Provide a Temporal query to read intermediate chunk states for debugging.
	•	Store final_decision_details JSON in document_classifications for auditability:
```json
{
  "aggregated_scores": {...},
  "top_class": "claim",
  "top_score": 0.83,
  "method": "aggregate",
  "fallback_used": false,
  "chunks_used": 16
}
```

---

## Temporal integration (activities / child workflows)

### Workflow: DocumentProcessingWorkflow orchestrates:
	1.	ocr_activity(document_id) → produce pages and raw text
	2.	chunk_activity(document_id) → produce chunks & store document_chunks
	3.	Parallel child workflows or activities:
	•	For each chunk: normalize_and_extract_signals_activity(chunk_id) → calls LLM, writes document_chunks.normalized_text and chunk_classification_signals.
	4.	aggregate_signals_activity(document_id) → reads chunk signals, runs aggregator, decides final label or fallback.
	5.	classification_fallback_activity(document_id, summary) → (optional LLM call if aggregator uncertain)
	6.	persist_classification_activity(document_id, result)
	7.	downstream: vector_index_activity, kg_update_activity, finalize workflow.

Important: Use Temporal for parallel chunk child workflows. Child workflows grant durable, retriable parallelism and good visibility.

---

### Prompt engineering & calibration tips
	•	Use consistent prompt template across all chunks and LLM calls to keep per-chunk signals comparable.
	•	Instruct the LLM to output exact JSON and nothing else to simplify parsing.
	•	Calibrate a small labeled set (50–200 documents) to tune thresholds (ACCEPT_THRESHOLD, REVIEW_THRESHOLD).
	•	Optionally include a short schema hint in chunk call: e.g., "This chunk is from page X of a commercial policy." to give localized context.

---

### Fallback / Human-in-the-loop
	•	If aggregation confidence < REVIEW_THRESHOLD, either:
	•	Run fallback single LLM classification on aggregated summary, or
	•	Create a human_review_tasks entry (if business requires human validation).
	•	For critical fields (policy number, claim number) route to human if entity extraction confidence is low even when classification is high.

---

### Performance & cost considerations
	•	Per-chunk LLM calls: fully parallelize via Temporal worker pool (N workers). Use caching (hash chunk -> result) to avoid repeated calls.
	•	Fallback LLM calls: rarely used if aggregator performs well — drastically reduces LLM usage vs. naïve per-document full classification.
	•	Local rule pre-filter: do a quick rule pass before LLM to skip normalization LLM for trivial chunks (e.g., blank pages, scanned images).

---

## Example: end-to-end JSON flow for one chunk (sample)
```json
{
  "chunk_id": "uuid-123",
  "normalized_text": "The insured, ACME Corp, reported a loss dated 2025-01-10...",
  "signals": {
    "policy": 0.10,
    "claim": 0.82,
    "submission": 0.03,
    "SOV": 0.01
  },
  "keywords": ["loss date", "insured", "claim number"],
  "entities": {"claim_number":"CLM-2025-001", "loss_date":"2025-01-10"},
  "model_name":"gpt-4o-mini",
  "model_confidence":0.87
}
```

---

Testing & Calibration plan
	1.	Collect a labeled holdout set (200 docs across classes).
	2.	Run full pipeline; store per-chunk signals and aggregated results.
	3.	Compute confusion matrix; adjust weights and thresholds.
	4.	If aggregator uncertain often, add summary fallback or tune chunk prompts for better signals.
	5.	Iterate until acceptable precision/recall (e.g., >0.9 for major classes).

---

Summary checklist (implementation)
	•	Add document_chunks and chunk_classification_signals tables.
	•	Update chunk LLM prompt to include signal extraction.
	•	Implement normalize_and_extract_signals_activity as a Temporal child workflow (parallelized).
	•	Implement server-side aggregate_signals_activity with configurable weights & thresholds.
	•	Implement fallback single-LRM classification on aggregated summary (optional).
	•	Persist aggregated decision & full audit JSON into document_classifications.
	•	Build monitoring: per-chunk latency, number of fallbacks, final confidence distribution.
	•	Tune thresholds using labeled dataset.

---

Next artifacts I can generate (if you want)
	•	Concrete Temporal workflow .mdc with activities and sample code (TS/Python).
	•	Exact per-chunk LLM prompt variations for Mistral / OpenAI / Anthropic.
	•	SQL migration file for new tables.
	•	Aggregator code example (Python).
	•	Unit tests and evaluation script to calibrate thresholds.
