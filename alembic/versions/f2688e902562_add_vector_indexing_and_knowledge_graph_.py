"""Add vector indexing and knowledge graph support

Revision ID: f2688e902562
Revises: 
Create Date: 2025-11-23 19:40:52.338546

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = 'f2688e902562'
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('canonical_entities',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('entity_type', sa.String(), nullable=False, comment='POLICY, CLAIM, INSURED, ADDRESS, CARRIER, etc.'),
    sa.Column('canonical_key', sa.String(), nullable=False, comment='Unique identifier: policy number, claim number, etc.'),
    sa.Column('attributes', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Entity properties: name, dates, amounts, etc.'),
    sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.Column('updated_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('entity_type', 'canonical_key', name='uq_entity_type_canonical_key'),
    comment='Canonical entities with unique (entity_type, canonical_key)'
    )
    op.create_table('graph_sync_state',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('source_table', sa.String(), nullable=False, comment='Table name: canonical_entities, entity_relationships, etc.'),
    sa.Column('source_id', sa.UUID(), nullable=False, comment='Record ID in source table'),
    sa.Column('neo4j_node_id', sa.String(), nullable=True, comment='Neo4j internal node/relationship ID'),
    sa.Column('last_sync_at', sa.TIMESTAMP(timezone=True), nullable=True, comment='Last successful sync time'),
    sa.Column('sync_status', sa.String(), nullable=False, comment='pending, synced, failed'),
    sa.Column('sync_error', sa.Text(), nullable=True, comment='Error message if sync failed'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('entity_relationships',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('source_entity_id', sa.UUID(), nullable=True),
    sa.Column('target_entity_id', sa.UUID(), nullable=True),
    sa.Column('relationship_type', sa.String(), nullable=False, comment='HAS_CLAIM, INSURED_BY, HAS_COVERAGE, LOCATED_AT, etc.'),
    sa.Column('attributes', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Relationship metadata'),
    sa.Column('confidence', sa.Numeric(), nullable=True, comment='Extraction confidence'),
    sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.ForeignKeyConstraint(['source_entity_id'], ['canonical_entities.id'], ),
    sa.ForeignKeyConstraint(['target_entity_id'], ['canonical_entities.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('document_entity_links',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('document_id', sa.UUID(), nullable=False),
    sa.Column('canonical_entity_id', sa.UUID(), nullable=False),
    sa.Column('confidence', sa.Numeric(), nullable=True, comment='Linking confidence'),
    sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.ForeignKeyConstraint(['canonical_entity_id'], ['canonical_entities.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('loss_run_claims',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('document_id', sa.UUID(), nullable=True),
    sa.Column('claim_number', sa.String(), nullable=True, comment='Claim identifier'),
    sa.Column('insured_name', sa.String(), nullable=True, comment='Insured party name'),
    sa.Column('loss_date', sa.Date(), nullable=True, comment='Date of loss'),
    sa.Column('cause_of_loss', sa.String(), nullable=True, comment='Loss cause/type'),
    sa.Column('incurred_amount', sa.Numeric(), nullable=True, comment='Total incurred'),
    sa.Column('paid_amount', sa.Numeric(), nullable=True, comment='Amount paid'),
    sa.Column('reserve_amount', sa.Numeric(), nullable=True, comment='Reserve amount'),
    sa.Column('status', sa.String(), nullable=True, comment='Claim status'),
    sa.Column('additional_data', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Additional fields'),
    sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('sov_items',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('document_id', sa.UUID(), nullable=True),
    sa.Column('location_number', sa.String(), nullable=True, comment='Location identifier'),
    sa.Column('building_number', sa.String(), nullable=True, comment='Building identifier'),
    sa.Column('description', sa.Text(), nullable=True, comment='Property description'),
    sa.Column('construction_type', sa.String(), nullable=True, comment='Construction class'),
    sa.Column('occupancy', sa.String(), nullable=True, comment='Occupancy type'),
    sa.Column('year_built', sa.Integer(), nullable=True, comment='Year of construction'),
    sa.Column('square_footage', sa.Integer(), nullable=True, comment='Building size'),
    sa.Column('limit', sa.Numeric(), nullable=True, comment='Coverage limit'),
    sa.Column('deductible', sa.Numeric(), nullable=True, comment='Deductible amount'),
    sa.Column('additional_data', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Additional fields'),
    sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.ForeignKeyConstraint(['document_id'], ['documents.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('chunk_embeddings',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('chunk_id', sa.UUID(), nullable=False),
    sa.Column('embedding_model', sa.String(), nullable=False, comment='Model name: text-embedding-3-large, etc.'),
    sa.Column('embedding_version', sa.String(), nullable=False, comment='Model version for tracking updates'),
    sa.Column('embedding_dimension', sa.Integer(), nullable=False, comment='Vector dimension: 1536, 3072, etc.'),
    sa.Column('embedding', postgresql.JSONB(astext_type=sa.Text()), nullable=False, comment='JSONB array of floats representing the vector'),
    sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.ForeignKeyConstraint(['chunk_id'], ['normalized_chunks.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('chunk_entity_links',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('chunk_id', sa.UUID(), nullable=False),
    sa.Column('canonical_entity_id', sa.UUID(), nullable=False),
    sa.Column('confidence', sa.Numeric(), nullable=True, comment='Linking confidence'),
    sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.ForeignKeyConstraint(['canonical_entity_id'], ['canonical_entities.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['chunk_id'], ['normalized_chunks.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('chunk_entity_mentions',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('chunk_id', sa.UUID(), nullable=False),
    sa.Column('entity_type', sa.String(), nullable=False, comment='Type of entity mentioned'),
    sa.Column('raw_value', sa.Text(), nullable=False, comment='Original text as it appears in chunk'),
    sa.Column('normalized_value', sa.Text(), nullable=True, comment='Cleaned/standardized value'),
    sa.Column('confidence', sa.Numeric(), nullable=True, comment='Detection confidence (0.0-1.0)'),
    sa.Column('span_start', sa.Integer(), nullable=True, comment='Character offset start'),
    sa.Column('span_end', sa.Integer(), nullable=True, comment='Character offset end'),
    sa.Column('created_at', sa.TIMESTAMP(timezone=True), server_default='NOW()', nullable=False),
    sa.ForeignKeyConstraint(['chunk_id'], ['normalized_chunks.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('embedding_sync_state',
    sa.Column('id', sa.UUID(), nullable=False),
    sa.Column('chunk_id', sa.UUID(), nullable=False),
    sa.Column('last_embedding_at', sa.TIMESTAMP(timezone=True), nullable=True, comment='When embedding was last generated'),
    sa.Column('embedding_model', sa.String(), nullable=True, comment='Model used'),
    sa.Column('embedding_version', sa.String(), nullable=True, comment='Version used'),
    sa.Column('sync_status', sa.String(), nullable=False, comment='pending, synced, failed'),
    sa.Column('sync_error', sa.Text(), nullable=True, comment='Error message if sync failed'),
    sa.ForeignKeyConstraint(['chunk_id'], ['normalized_chunks.id'], ondelete='CASCADE'),
    sa.PrimaryKeyConstraint('id')
    )
    op.add_column('chunk_classification_signals', sa.Column('model_version', sa.String(), nullable=True, comment='Classifier model version'))
    op.add_column('chunk_classification_signals', sa.Column('pipeline_run_id', sa.UUID(), nullable=True, comment='Links to specific pipeline execution'))
    op.add_column('chunk_classification_signals', sa.Column('source_stage', sa.String(), nullable=True, comment='Pipeline stage: classification'))
    op.add_column('document_chunks', sa.Column('section_type', sa.String(), nullable=True, comment='High-level section: Declarations, Coverages, etc.'))
    op.add_column('document_chunks', sa.Column('subsection_type', sa.String(), nullable=True, comment='Fine-grained subsection: Named Insured, Limits, etc.'))
    op.add_column('document_chunks', sa.Column('stable_chunk_id', sa.String(), nullable=True, comment='Deterministic ID: doc_{document_id}_p{page}_c{chunk}'))
    op.create_unique_constraint('uq_document_chunks_stable_chunk_id', 'document_chunks', ['stable_chunk_id'])
    op.add_column('normalized_chunks', sa.Column('extracted_fields', postgresql.JSONB(astext_type=sa.Text()), nullable=True))
    op.add_column('normalized_chunks', sa.Column('entities', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Structured entity mentions extracted from chunk'))
    op.add_column('normalized_chunks', sa.Column('relationships', postgresql.JSONB(astext_type=sa.Text()), nullable=True, comment='Structured relationships between entities'))
    op.add_column('normalized_chunks', sa.Column('model_version', sa.String(), nullable=True, comment='LLM/normalizer version for provenance'))
    op.add_column('normalized_chunks', sa.Column('quality_score', sa.Numeric(), nullable=True, comment='Confidence/quality metric for normalization'))
    op.add_column('ocr_results', sa.Column('model_version', sa.String(), nullable=True, comment='OCR engine version'))
    op.add_column('ocr_results', sa.Column('pipeline_run_id', sa.UUID(), nullable=True, comment='Links to specific pipeline execution'))
    op.add_column('ocr_results', sa.Column('source_stage', sa.String(), nullable=True, comment='Pipeline stage: ocr'))
    
    # Create indexes for better query performance
    op.create_index('idx_chunk_entity_chunk', 'chunk_entity_mentions', ['chunk_id'])
    op.create_index('idx_chunk_embeddings_chunk', 'chunk_embeddings', ['chunk_id'])
    op.create_index('idx_document_entity_document', 'document_entity_links', ['document_id'])
    op.create_index('idx_canonical_entities_key', 'canonical_entities', ['entity_type', 'canonical_key'])
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    # Drop indexes
    op.drop_index('idx_canonical_entities_key', 'canonical_entities')
    op.drop_index('idx_document_entity_document', 'document_entity_links')
    op.drop_index('idx_chunk_embeddings_chunk', 'chunk_embeddings')
    op.drop_index('idx_chunk_entity_chunk', 'chunk_entity_mentions')
    
    op.drop_column('ocr_results', 'source_stage')
    op.drop_column('ocr_results', 'pipeline_run_id')
    op.drop_column('ocr_results', 'model_version')
    op.drop_column('normalized_chunks', 'quality_score')
    op.drop_column('normalized_chunks', 'model_version')
    op.drop_column('normalized_chunks', 'relationships')
    op.drop_column('normalized_chunks', 'entities')
    op.drop_column('normalized_chunks', 'extracted_fields')
    op.drop_constraint('document_chunks_stable_chunk_id_key', 'document_chunks', type_='unique')
    op.drop_column('document_chunks', 'stable_chunk_id')
    op.drop_column('document_chunks', 'subsection_type')
    op.drop_column('document_chunks', 'section_type')
    op.drop_column('chunk_classification_signals', 'source_stage')
    op.drop_column('chunk_classification_signals', 'pipeline_run_id')
    op.drop_column('chunk_classification_signals', 'model_version')
    op.drop_table('embedding_sync_state')
    op.drop_table('chunk_entity_mentions')
    op.drop_table('chunk_entity_links')
    op.drop_table('chunk_embeddings')
    op.drop_table('sov_items')
    op.drop_table('loss_run_claims')
    op.drop_table('document_entity_links')
    op.drop_table('entity_relationships')
    op.drop_table('graph_sync_state')
    op.drop_table('canonical_entities')
    # ### end Alembic commands ###
