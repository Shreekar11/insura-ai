# Example .env configuration for LLM providers

# LLM Provider Configuration
# Choose between "gemini" or "openrouter"
LLM_PROVIDER=gemini

# Gemini Configuration (required if LLM_PROVIDER=gemini)
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.0-flash

# OpenRouter Configuration (required if LLM_PROVIDER=openrouter)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_API_URL=https://openrouter.ai/api/v1/chat/completions
OPENROUTER_MODEL=google/gemini-2.0-flash-001

# LLM Fallback Configuration
# Enable automatic fallback to Gemini if OpenRouter fails
ENABLE_LLM_FALLBACK=false

# Other existing configuration...
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_API_URL=https://api.mistral.ai/v1/ocr
MISTRAL_MODEL=mistral-ocr-latest

DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/insura_ai